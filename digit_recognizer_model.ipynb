{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sK83270bmhN",
    "colab_type": "text"
   },
   "source": [
    "# CNN Model for audio digit recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOW4BjUib1go",
    "colab_type": "text"
   },
   "source": [
    "** We first download the speech command dataset of the tensorflow team from where we will use the digit speech commands.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "B16R1SG6g-4o",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz  \n",
    "!tar xf speech_commands_v0.01.tar.gz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmq82YlacPvH",
    "colab_type": "text"
   },
   "source": [
    "**Next we import necessary modules.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UkiLivWxnfpx",
    "colab_type": "code",
    "outputId": "fac9e0f7-e44a-4350-81c4-c7413eec84a1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36.0
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import IPython\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Add, Dropout, Dense, GRU, Bidirectional, Masking, TimeDistributed, LSTM, Conv1D, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import layer_utils, plot_model, to_categorical\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.applications import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.initializers import glorot_uniform\n",
    "from scipy.fftpack import fft\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "import copy\n",
    "from keras import backend as k\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import butter, lfilter\n",
    "import scipy.ndimage\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4fMgmSncgcW",
    "colab_type": "text"
   },
   "source": [
    "**We define several functions to convert wav audio to their spectrogram representation and also vice versa.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "lrgqMlQDZL83",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# The credit for this section of the code goes to Tim Sainburg: https://timsainb.github.io/spectrograms-mfccs-and-inversion-in-python.html\n",
    "# Most of the Spectrograms and Inversion are taken from: https://gist.github.com/kastnerkyle/179d6e9a88202ab0a2fe\n",
    "\n",
    "def overlap(X, window_size, window_step):\n",
    "    \"\"\"\n",
    "    Create an overlapped version of X\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape=(n_samples,)\n",
    "        Input signal to window and overlap\n",
    "    window_size : int\n",
    "        Size of windows to take\n",
    "    window_step : int\n",
    "        Step size between windows\n",
    "    Returns\n",
    "    -------\n",
    "    X_strided : shape=(n_windows, window_size)\n",
    "        2D array of overlapped X\n",
    "    \"\"\"\n",
    "    if window_size % 2 != 0:\n",
    "        raise ValueError(\"Window size must be even!\")\n",
    "    # Make sure there are an even number of windows before stridetricks\n",
    "    append = np.zeros((window_size - len(X) % window_size))\n",
    "    X = np.hstack((X, append))\n",
    "\n",
    "    ws = window_size\n",
    "    ss = window_step\n",
    "    a = X\n",
    "\n",
    "    valid = len(a) - ws\n",
    "    nw = (valid) // ss\n",
    "    out = np.ndarray((nw,ws),dtype = a.dtype)\n",
    "\n",
    "    for i in range(nw):\n",
    "        # \"slide\" the window along the samples\n",
    "        start = i * ss\n",
    "        stop = start + ws\n",
    "        out[i] = a[start : stop]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def stft(X, fftsize=128, step=65, mean_normalize=True, real=False,\n",
    "         compute_onesided=True):\n",
    "    \"\"\"\n",
    "    Compute STFT for 1D real valued input X\n",
    "    \"\"\"\n",
    "    if real:\n",
    "        local_fft = np.fft.rfft\n",
    "        cut = -1\n",
    "    else:\n",
    "        local_fft = np.fft.fft\n",
    "        cut = None\n",
    "    if compute_onesided:\n",
    "        cut = fftsize // 2\n",
    "    if mean_normalize:\n",
    "        X -= X.mean()\n",
    "\n",
    "    X = overlap(X, fftsize, step)\n",
    "    \n",
    "    size = fftsize\n",
    "    win = 0.54 - .46 * np.cos(2 * np.pi * np.arange(size) / (size - 1))\n",
    "    X = X * win[None]\n",
    "    X = local_fft(X)[:, :cut]\n",
    "    return X\n",
    "\n",
    "def pretty_spectrogram(d,log = True, thresh= 5, fft_size = 512, step_size = 64):\n",
    "    \"\"\"\n",
    "    creates a spectrogram\n",
    "    log: take the log of the spectrgram\n",
    "    thresh: threshold minimum power for log spectrogram\n",
    "    \"\"\"\n",
    "    specgram = np.abs(stft(d, fftsize=fft_size, step=step_size, real=False,\n",
    "        compute_onesided=True))\n",
    "  \n",
    "    if log == True:\n",
    "        specgram /= specgram.max() # volume normalize to max 1\n",
    "        specgram = np.log10(specgram) # take log\n",
    "        specgram[specgram < -thresh] = -thresh # set anything less than the threshold as the threshold\n",
    "    else:\n",
    "        specgram[specgram < thresh] = thresh # set anything less than the threshold as the threshold\n",
    "    \n",
    "    return specgram\n",
    "\n",
    "# Also mostly modified or taken from https://gist.github.com/kastnerkyle/179d6e9a88202ab0a2fe\n",
    "def invert_pretty_spectrogram(X_s, log = True, fft_size = 512, step_size = 512/4, n_iter = 10):\n",
    "    \n",
    "    if log == True:\n",
    "        X_s = np.power(10, X_s)\n",
    "\n",
    "    X_s = np.concatenate([X_s, X_s[:, ::-1]], axis=1)\n",
    "    X_t = iterate_invert_spectrogram(X_s, fft_size, step_size, n_iter=n_iter)\n",
    "    return X_t\n",
    "\n",
    "def iterate_invert_spectrogram(X_s, fftsize, step, n_iter=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Under MSR-LA License\n",
    "    Based on MATLAB implementation from Spectrogram Inversion Toolbox\n",
    "    References\n",
    "    ----------\n",
    "    D. Griffin and J. Lim. Signal estimation from modified\n",
    "    short-time Fourier transform. IEEE Trans. Acoust. Speech\n",
    "    Signal Process., 32(2):236-243, 1984.\n",
    "    Malcolm Slaney, Daniel Naar and Richard F. Lyon. Auditory\n",
    "    Model Inversion for Sound Separation. Proc. IEEE-ICASSP,\n",
    "    Adelaide, 1994, II.77-80.\n",
    "    Xinglei Zhu, G. Beauregard, L. Wyse. Real-Time Signal\n",
    "    Estimation from Modified Short-Time Fourier Transform\n",
    "    Magnitude Spectra. IEEE Transactions on Audio Speech and\n",
    "    Language Processing, 08/2007.\n",
    "    \"\"\"\n",
    "    reg = np.max(X_s) / 1E8\n",
    "    X_best = copy.deepcopy(X_s)\n",
    "    for i in range(n_iter):\n",
    "        if verbose:\n",
    "            print(\"Runnning iter %i\" % i)\n",
    "        if i == 0:\n",
    "            X_t = invert_spectrogram(X_best, step, calculate_offset=True,\n",
    "                                     set_zero_phase=True)\n",
    "        else:\n",
    "            # Calculate offset was False in the MATLAB version\n",
    "            # but in mine it massively improves the result\n",
    "            # Possible bug in my impl?\n",
    "            X_t = invert_spectrogram(X_best, step, calculate_offset=True,\n",
    "                                     set_zero_phase=False)\n",
    "        est = stft(X_t, fftsize=fftsize, step=step, compute_onesided=False)\n",
    "        phase = est / np.maximum(reg, np.abs(est))\n",
    "        X_best = X_s * phase[:len(X_s)]\n",
    "    X_t = invert_spectrogram(X_best, step, calculate_offset=True,\n",
    "                             set_zero_phase=False)\n",
    "    return np.real(X_t)\n",
    "\n",
    "def invert_spectrogram(X_s, step, calculate_offset=True, set_zero_phase=True):\n",
    "    \"\"\"\n",
    "    Under MSR-LA License\n",
    "    Based on MATLAB implementation from Spectrogram Inversion Toolbox\n",
    "    References\n",
    "    ----------\n",
    "    D. Griffin and J. Lim. Signal estimation from modified\n",
    "    short-time Fourier transform. IEEE Trans. Acoust. Speech\n",
    "    Signal Process., 32(2):236-243, 1984.\n",
    "    Malcolm Slaney, Daniel Naar and Richard F. Lyon. Auditory\n",
    "    Model Inversion for Sound Separation. Proc. IEEE-ICASSP,\n",
    "    Adelaide, 1994, II.77-80.\n",
    "    Xinglei Zhu, G. Beauregard, L. Wyse. Real-Time Signal\n",
    "    Estimation from Modified Short-Time Fourier Transform\n",
    "    Magnitude Spectra. IEEE Transactions on Audio Speech and\n",
    "    Language Processing, 08/2007.\n",
    "    \"\"\"\n",
    "    size = int(X_s.shape[1] // 2)\n",
    "    wave = np.zeros((X_s.shape[0] * step + size))\n",
    "    # Getting overflow warnings with 32 bit...\n",
    "    wave = wave.astype('float64')\n",
    "    total_windowing_sum = np.zeros((X_s.shape[0] * step + size))\n",
    "    win = 0.54 - .46 * np.cos(2 * np.pi * np.arange(size) / (size - 1))\n",
    "\n",
    "    est_start = int(size // 2) - 1\n",
    "    est_end = est_start + size\n",
    "    for i in range(X_s.shape[0]):\n",
    "        wave_start = int(step * i)\n",
    "        wave_end = wave_start + size\n",
    "        if set_zero_phase:\n",
    "            spectral_slice = X_s[i].real + 0j\n",
    "        else:\n",
    "            # already complex\n",
    "            spectral_slice = X_s[i]\n",
    "\n",
    "        # Don't need fftshift due to different impl.\n",
    "        wave_est = np.real(np.fft.ifft(spectral_slice))[::-1]\n",
    "        if calculate_offset and i > 0:\n",
    "            offset_size = size - step\n",
    "            if offset_size <= 0:\n",
    "                print(\"WARNING: Large step size >50\\% detected! \"\n",
    "                      \"This code works best with high overlap - try \"\n",
    "                      \"with 75% or greater\")\n",
    "                offset_size = step\n",
    "            offset = xcorr_offset(wave[wave_start:wave_start + offset_size],\n",
    "                                  wave_est[est_start:est_start + offset_size])\n",
    "        else:\n",
    "            offset = 0\n",
    "        wave[wave_start:wave_end] += win * wave_est[\n",
    "            est_start - offset:est_end - offset]\n",
    "        total_windowing_sum[wave_start:wave_end] += win\n",
    "    wave = np.real(wave) / (total_windowing_sum + 1E-6)\n",
    "    return wave\n",
    "\n",
    "def xcorr_offset(x1, x2):\n",
    "    \"\"\"\n",
    "    Under MSR-LA License\n",
    "    Based on MATLAB implementation from Spectrogram Inversion Toolbox\n",
    "    References\n",
    "    ----------\n",
    "    D. Griffin and J. Lim. Signal estimation from modified\n",
    "    short-time Fourier transform. IEEE Trans. Acoust. Speech\n",
    "    Signal Process., 32(2):236-243, 1984.\n",
    "    Malcolm Slaney, Daniel Naar and Richard F. Lyon. Auditory\n",
    "    Model Inversion for Sound Separation. Proc. IEEE-ICASSP,\n",
    "    Adelaide, 1994, II.77-80.\n",
    "    Xinglei Zhu, G. Beauregard, L. Wyse. Real-Time Signal\n",
    "    Estimation from Modified Short-Time Fourier Transform\n",
    "    Magnitude Spectra. IEEE Transactions on Audio Speech and\n",
    "    Language Processing, 08/2007.\n",
    "    \"\"\"\n",
    "    x1 = x1 - x1.mean()\n",
    "    x2 = x2 - x2.mean()\n",
    "    frame_size = len(x2)\n",
    "    half = frame_size // 2\n",
    "    corrs = np.convolve(x1.astype('float32'), x2[::-1].astype('float32'))\n",
    "    corrs[:half] = -1E30\n",
    "    corrs[-half:] = -1E30\n",
    "    offset = corrs.argmax() - len(x1)\n",
    "    return offset\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPuU8pcleZ1k",
    "colab_type": "text"
   },
   "source": [
    "**Now we initialize some helper functions and necessary parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "zp4Ow9NwZoob",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def pad_audio(samples, t=1, L=8000):\n",
    "    if len(samples) >= t*L: return samples\n",
    "    else: return np.pad(samples, pad_width=(L - len(samples), 0), mode='constant', constant_values=(0, 0))\n",
    "\n",
    "def chop_audio(samples, t=1, L=8000):\n",
    "    if len(samples) <= t*L: return samples\n",
    "    else: return samples[0:L] \n",
    "    \n",
    "def get_data(paths, label):\n",
    "  \n",
    "  new_sample_rate = 8000\n",
    "  max_len=20\n",
    "  X = []\n",
    "  Y = []\n",
    " \n",
    "  for i, fname in enumerate(paths):\n",
    "    sample_rate, samples = wavfile.read(fname)    \n",
    "    resampled = signal.resample(samples, int(8000 / sample_rate * samples.shape[0]))\n",
    "    samples = pad_audio(resampled)\n",
    "    specgram = pretty_spectrogram(samples.astype('float64'), fft_size = fft_size, \n",
    "                                   step_size = int(step_size), log = True, thresh = spec_thresh)\n",
    "    Y.append(label)\n",
    "    X.append(specgram)\n",
    "    \n",
    "    if i==len(paths)-1:\n",
    "        end='\\n'\n",
    "    else: end='\\r'\n",
    "    print('processed {}/{}'.format(i+1,len(paths)),end=end)\n",
    "    \n",
    "  Y = to_categorical(Y, 10)\n",
    "  \n",
    "  return X, Y  \n",
    "\n",
    "### Parameters ###\n",
    "L = 8000 #sample rate\n",
    "fft_size = 320 # window size for the FFT\n",
    "step_size = fft_size/6 # distance to slide along the window (in time)\n",
    "spec_thresh = 4 # threshold for spectrograms (lower filters out more noise)\n",
    "lowcut = 500 # Hz # Low cut for our butter bandpass filter\n",
    "highcut = 8000 # Hz # High cut for our butter bandpass filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1ppv7rqfn7o",
    "colab_type": "text"
   },
   "source": [
    "**Below lines are useful to extract the paths of all the digit audio files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "DfSwVYMSHE-D",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "data_dir=os.path.join('')\n",
    "paths_a=glob.glob(os.path.join(data_dir,'one','*.wav'))\n",
    "paths_b=glob.glob(os.path.join(data_dir,'two','*.wav'))\n",
    "paths_c=glob.glob(os.path.join(data_dir,'three','*.wav'))\n",
    "paths_d=glob.glob(os.path.join(data_dir,'four','*.wav'))\n",
    "paths_e=glob.glob(os.path.join(data_dir,'five','*.wav'))\n",
    "paths_f=glob.glob(os.path.join(data_dir,'six','*.wav'))\n",
    "paths_g=glob.glob(os.path.join(data_dir,'seven','*.wav'))\n",
    "paths_h=glob.glob(os.path.join(data_dir,'eight','*.wav'))\n",
    "paths_i=glob.glob(os.path.join(data_dir,'nine','*.wav'))\n",
    "paths_j=glob.glob(os.path.join(data_dir,'zero','*.wav'))\n",
    "paths_all=paths_a+paths_b+paths_c+paths_d+paths_e+paths_f+paths_g+paths_h+paths_i+paths_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3emZ5rMfzAp",
    "colab_type": "text"
   },
   "source": [
    "**Now we read and preprocess all the digit speech commands and convert them to their respective spectrograms.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Jogn3jhtnreL",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "X_a, Y_a=get_data(paths_a,1)\n",
    "X_b, Y_b=get_data(paths_b,2)\n",
    "X = np.concatenate((X_a, X_b), axis=0)\n",
    "Y = np.concatenate((Y_a, Y_b), axis=0)\n",
    "X_a, Y_a=get_data(paths_c,3)\n",
    "X_b, Y_b=get_data(paths_d,4)\n",
    "X = np.concatenate((X, X_a, X_b), axis=0)\n",
    "Y = np.concatenate((Y, Y_a, Y_b), axis=0)\n",
    "X_a, Y_a=get_data(paths_e,5)\n",
    "X_b, Y_b=get_data(paths_f,6)\n",
    "X = np.concatenate((X, X_a, X_b), axis=0)\n",
    "Y = np.concatenate((Y, Y_a, Y_b), axis=0)\n",
    "X_a, Y_a=get_data(paths_g,7)\n",
    "X_b, Y_b=get_data(paths_h,8)\n",
    "X = np.concatenate((X, X_a, X_b), axis=0)\n",
    "Y = np.concatenate((Y, Y_a, Y_b), axis=0)\n",
    "del X_b\n",
    "del Y_b\n",
    "X_a, Y_a=get_data(paths_i,9)\n",
    "X = np.concatenate((X, X_a), axis=0)\n",
    "Y = np.concatenate((Y, Y_a), axis=0)\n",
    "X_a, Y_a=get_data(paths_j,0)\n",
    "X = np.concatenate((X, X_a), axis=0)\n",
    "Y = np.concatenate((Y, Y_a), axis=0)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TdvqQl6gMCB",
    "colab_type": "text"
   },
   "source": [
    "**After preprocessing we split the data to train and validation sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "XkYriAirI9sj",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.15, random_state=5, shuffle=True)\n",
    "X_train = X_train.reshape(X_train.shape[0],X_train.shape[1], X_train.shape[2],1)\n",
    "X_val = X_val.reshape(X_val.shape[0],X_val.shape[1], X_val.shape[2],1)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KCGKSVhgTGp",
    "colab_type": "text"
   },
   "source": [
    "**Now we initialize the function for declaring the CNN model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "FH7bQZ1sUaT0",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def custom():\n",
    "  \n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(16, kernel_size=(7, 7), name='one', activation='relu', padding='same', input_shape=(X_train.shape[1], X_train.shape[2], 1), kernel_initializer=glorot_uniform(seed=0)))\n",
    "  model.add(BatchNormalization(axis=3))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "  \n",
    "  model.add(Conv2D(16, kernel_size=(5, 5), name='two', activation='relu', padding='same', kernel_initializer=glorot_uniform(seed=0)))\n",
    "  model.add(BatchNormalization(axis=3))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "  \n",
    "  model.add(Conv2D(32, kernel_size=(3, 3), name='three', activation='relu', padding='same', kernel_initializer=glorot_uniform(seed=0)))\n",
    "  model.add(BatchNormalization(axis=3))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "  \n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(256, activation='relu'))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(10, activation='softmax'))\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXJpTI5yhY-o",
    "colab_type": "text"
   },
   "source": [
    "**Finally we train our CNN model. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "sQu8iwFpbIc5",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "K.tensorflow_backend.clear_session()\n",
    "\n",
    "model = custom()\n",
    "model.summary()\n",
    "K.set_value(model.optimizer.lr,0.001)\n",
    "model_check_point = ModelCheckpoint(filepath='model.hdf5', save_best_only=True)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "h=model.fit(x=(X_train), y=Y_train, batch_size=32,\n",
    "            epochs=15, \n",
    "            verbose=1,          \n",
    "            validation_data=((X_val),Y_val),\n",
    "            shuffle=True,\n",
    "            callbacks=[learning_rate_reduction,model_check_point]\n",
    "            )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pWKWgMHhpb5",
    "colab_type": "text"
   },
   "source": [
    "**After training we save the weights and divide the model to get different layer outputs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "I7NyV5DIIp-4",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "model.save_weights('model_weights.h5')\n",
    "model.pop()\n",
    "model.pop()\n",
    "model.pop()\n",
    "model.pop()\n",
    "model.pop()\n",
    "model.pop()\n",
    "model.pop()\n",
    "model.save('third_layer.hdf5')\n",
    "\n",
    "model.pop()\n",
    "model.pop()\n",
    "model.pop()\n",
    "model.pop()\n",
    "model.save('second_layer.hdf5')\n",
    "\n",
    "model.pop()\n",
    "model.pop()\n",
    "model.pop()\n",
    "model.pop()\n",
    "model.save('first_layer.hdf5')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "digit_recognizer.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
